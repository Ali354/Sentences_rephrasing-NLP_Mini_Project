{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5fad6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform standard imports\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7d9e057",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerEncoder():\n",
    "    def __init__(self):\n",
    "        self.ner_dict = {}\n",
    "        self.label_index = 0\n",
    "        \n",
    "        \n",
    "    def translate(self, label, word):\n",
    "        self.label_index += 1\n",
    "        if label == 'NORP':\n",
    "            self.ner_dict[\"Group\" + str(self.label_index)] = word\n",
    "            return \"Group\" + str(self.label_index)\n",
    "        if label == 'FAC':\n",
    "            self.ner_dict[\"Place\" + str(self.label_index)] = word\n",
    "            return \"Place\" + str(self.label_index)\n",
    "        if label == 'ORG':\n",
    "            self.ner_dict[\"Organization\" + str(self.label_index)] = word\n",
    "            return \"Organization\" + str(self.label_index)\n",
    "        if label == 'GPE':\n",
    "            self.ner_dict[\"State\" + str(self.label_index)] = word\n",
    "            return \"State\" + str(self.label_index)\n",
    "        if label == 'LOC':\n",
    "            self.ner_dict[\"Location\" + str(self.label_index)] = word\n",
    "            return \"Location\" + str(self.label_index)\n",
    "        if label == 'PERCENT':\n",
    "            self.ner_dict[\"Percentage\" + str(self.label_index)] = word\n",
    "            return \"Percentage\" + str(self.label_index)\n",
    "        if label == 'ORDINAL':\n",
    "            self.ner_dict[\"Number\" + str(self.label_index)] = word\n",
    "            return \"Number\" + str(self.label_index)\n",
    "        if label == 'CARDINAL':\n",
    "            self.ner_dict[\"first\" + str(self.label_index)] = word\n",
    "            return \"first\" + str(self.label_index)\n",
    "        else:\n",
    "            self.ner_dict[label + str(self.label_index)] = word\n",
    "            return label + str(self.label_index)\n",
    "        \n",
    "    \n",
    "    def encode(self, doc):\n",
    "        res = \"\"\n",
    "        b = False\n",
    "        for i, token in enumerate(doc):\n",
    "            for ent in doc.ents:\n",
    "                if i == ent.start:\n",
    "                    b = True\n",
    "                    res += self.translate(ent.label_, ent.text) + \" \"\n",
    "                if i == ent.end:\n",
    "                    b = False\n",
    "            if b==False:\n",
    "                res += token.text +\" \"\n",
    "        return res\n",
    "    def decode(self, doc):\n",
    "        res = \"\"\n",
    "        for token in doc:\n",
    "            if token.text in self.ner_dict:\n",
    "                res += self.ner_dict[token.text] + \" \"\n",
    "            else:\n",
    "                res += token.text + \" \"\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d71a444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'ali ahmad ahmad loves Massachusetts Institute of Technology, he lives in the United States of America')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19d17ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON1 loves Organization2 , he lives in State3 \n",
      "ali ahmad ahmad loves Massachusetts Institute of Technology , he lives in the United States of America \n"
     ]
    }
   ],
   "source": [
    "encoder = NerEncoder()\n",
    "print(encoder.encode(doc))\n",
    "print(encoder.decode(nlp(encoder.encode(doc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0df8077c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node (from)-->   Relation     -->Node (to)\n",
      "\n",
      "loves             nsubj                 he\n",
      "loves              ROOT              loves\n",
      "job                poss                his\n",
      "loves              dobj                job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"6760479f8e32499fa68c898a13f4bd5d-0\" class=\"displacy\" width=\"750\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">he</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">loves</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">his</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">job</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6760479f8e32499fa68c898a13f4bd5d-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6760479f8e32499fa68c898a13f4bd5d-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6760479f8e32499fa68c898a13f4bd5d-0-1\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6760479f8e32499fa68c898a13f4bd5d-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6760479f8e32499fa68c898a13f4bd5d-0-2\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6760479f8e32499fa68c898a13f4bd5d-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,179.0 L583.0,167.0 567.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy \n",
    "print(f\"{'Node (from)-->':<15} {'Relation':^10} {'-->Node (to)':>15}\\n\")\n",
    "for token in doc:\n",
    "    print(\"{:<15} {:^10} {:>15}\".format(str(token.head.text), str(token.dep_), str(token.text)))\n",
    "displacy.render(doc, style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45b914f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node (from)-->   Relation     -->Node (to)\n",
      "\n",
      "loves             nsubj                 he\n",
      "loves              ROOT              loves\n",
      "job                poss                his\n",
      "loves              dobj                job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"96c2317cd5714773b45d7aa201b299ea-0\" class=\"displacy\" width=\"750\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">he</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">loves</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">his</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">job</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-96c2317cd5714773b45d7aa201b299ea-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-96c2317cd5714773b45d7aa201b299ea-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-96c2317cd5714773b45d7aa201b299ea-0-1\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-96c2317cd5714773b45d7aa201b299ea-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-96c2317cd5714773b45d7aa201b299ea-0-2\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-96c2317cd5714773b45d7aa201b299ea-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,179.0 L583.0,167.0 567.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "simplified_doc = nlp(encoder.encode(doc))\n",
    "from spacy import displacy \n",
    "print(f\"{'Node (from)-->':<15} {'Relation':^10} {'-->Node (to)':>15}\\n\")\n",
    "for token in simplified_doc:\n",
    "    print(\"{:<15} {:^10} {:>15}\".format(str(token.head.text), str(token.dep_), str(token.text)))\n",
    "displacy.render(simplified_doc, style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5b6b2601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synset_0\n",
      "a strong positive emotion of regard and affection\n",
      "synonyms: \n",
      "love\n",
      "antonynom is: Lemma('hate.n.01.hate')\n",
      "['his love for his work', 'children need a lot of love']\n",
      "***********************\n",
      "synset_1\n",
      "any object of warm affection or devotion\n",
      "synonyms: \n",
      "love\n",
      "passion\n",
      "['the theater was her first love', 'he has a passion for cock fighting']\n",
      "***********************\n",
      "synset_2\n",
      "a beloved person; used as terms of endearment\n",
      "synonyms: \n",
      "beloved\n",
      "dear\n",
      "dearest\n",
      "honey\n",
      "love\n",
      "no examples\n",
      "***********************\n",
      "synset_3\n",
      "a deep feeling of sexual desire and attraction\n",
      "synonyms: \n",
      "love\n",
      "sexual_love\n",
      "erotic_love\n",
      "['their love left them indifferent to their surroundings', 'she was his first love']\n",
      "***********************\n",
      "synset_4\n",
      "a score of zero in tennis or squash\n",
      "synonyms: \n",
      "love\n",
      "['it was 40 love']\n",
      "***********************\n",
      "synset_5\n",
      "sexual activities (often including sexual intercourse) between two people\n",
      "synonyms: \n",
      "sexual_love\n",
      "lovemaking\n",
      "making_love\n",
      "love\n",
      "love_life\n",
      "['his lovemaking disgusted her', \"he hadn't had any love in months\", 'he has a very complicated love life']\n",
      "***********************\n",
      "synset_6\n",
      "have a great affection or liking for\n",
      "synonyms: \n",
      "love\n",
      "antonynom is: Lemma('hate.v.01.hate')\n",
      "['I love French food', 'She loves her boss and works hard for him']\n",
      "***********************\n",
      "synset_7\n",
      "get pleasure from\n",
      "synonyms: \n",
      "love\n",
      "enjoy\n",
      "['I love cooking']\n",
      "***********************\n",
      "synset_8\n",
      "be enamored or in love with\n",
      "synonyms: \n",
      "love\n",
      "['She loves her husband deeply']\n",
      "***********************\n",
      "synset_9\n",
      "have sexual intercourse with\n",
      "synonyms: \n",
      "sleep_together\n",
      "roll_in_the_hay\n",
      "love\n",
      "make_out\n",
      "make_love\n",
      "sleep_with\n",
      "get_laid\n",
      "have_sex\n",
      "know\n",
      "do_it\n",
      "be_intimate\n",
      "have_intercourse\n",
      "have_it_away\n",
      "have_it_off\n",
      "screw\n",
      "fuck\n",
      "jazz\n",
      "eff\n",
      "hump\n",
      "lie_with\n",
      "bed\n",
      "have_a_go_at_it\n",
      "bang\n",
      "get_it_on\n",
      "bonk\n",
      "['This student sleeps with everyone in her dorm', 'Adam knew Eve', 'Were you ever intimate with this man?']\n",
      "***********************\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "x = wn.synsets('love')\n",
    "for i, synset in enumerate(x):\n",
    "    print(\"synset_\" + str(i))\n",
    "    print(synset.definition())\n",
    "    print(\"synonyms: \")\n",
    "    for l in synset.lemmas():\n",
    "        print(l.name())\n",
    "    if len(synset.lemmas()[0].antonyms()) > 0:\n",
    "        print(\"antonynom is:\", synset.lemmas()[0].antonyms()[0])\n",
    "    if len(synset.examples()) > 0:\n",
    "        print(synset.examples())\n",
    "    else:\n",
    "        print(\"no examples\")\n",
    "    print('***********************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f1e6edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f610bb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Loading the pre-trained BERT model\n",
    "###################################\n",
    "# Embeddings will be derived from\n",
    "# the outputs of this model\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True,\n",
    "                                  )\n",
    "\n",
    "# Setting up the tokenizer\n",
    "###################################\n",
    "# This is the same tokenizer that\n",
    "# was used in the model to generate \n",
    "# embeddings to ensure consistency\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20053634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_text_preparation(text, tokenizer):\n",
    "    \"\"\"Preparing the input for BERT\n",
    "    \n",
    "    Takes a string argument and performs\n",
    "    pre-processing like adding special tokens,\n",
    "    tokenization, tokens to ids, and tokens to\n",
    "    segment ids. All tokens are mapped to seg-\n",
    "    ment id = 1.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to be converted\n",
    "        tokenizer (obj): Tokenizer object\n",
    "            to convert text into BERT-re-\n",
    "            adable tokens and ids\n",
    "        \n",
    "    Returns:\n",
    "        list: List of BERT-readable tokens\n",
    "        obj: Torch tensor with token ids\n",
    "        obj: Torch tensor segment ids\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized_text = tokenizer.tokenize(marked_text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1]*len(indexed_tokens)\n",
    "\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "    return tokenized_text, tokens_tensor, segments_tensors\n",
    "    \n",
    "def get_bert_embeddings(tokens_tensor, segments_tensors, model):\n",
    "    \"\"\"Get embeddings from an embedding model\n",
    "    \n",
    "    Args:\n",
    "        tokens_tensor (obj): Torch tensor size [n_tokens]\n",
    "            with token ids for each token in text\n",
    "        segments_tensors (obj): Torch tensor size [n_tokens]\n",
    "            with segment ids for each token in text\n",
    "        model (obj): Embedding model to generate embeddings\n",
    "            from token and segment ids\n",
    "    \n",
    "    Returns:\n",
    "        list: List of list of floats of size\n",
    "            [n_tokens, n_embedding_dimensions]\n",
    "            containing embeddings for each token\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Gradient calculation id disabled\n",
    "    # Model is in inference mode\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensors)\n",
    "        # Removing the first hidden state\n",
    "        # The first state is the input state\n",
    "        hidden_states = outputs[2][1:]\n",
    "\n",
    "    # Getting embeddings from the final BERT layer\n",
    "    token_embeddings = hidden_states[-1]\n",
    "    # Collapsing the tensor into 1-dimension\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=0)\n",
    "    # Converting torchtensors to lists\n",
    "    list_token_embeddings = [token_embed.tolist() for token_embed in token_embeddings]\n",
    "\n",
    "    return list_token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce21b87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dep_encoder(doc, ind):\n",
    "    res = \"\"\n",
    "    allowed = [\"nsubj\", \"dobj\", \"cobj\", \"ROOT\", \"amod\", \"det\", \"poss\"]\n",
    "    for i, token in enumerate(doc):\n",
    "        if str(token.head.text) == str(doc[ind]):\n",
    "            if str(token.dep_) in allowed:\n",
    "                res += str(token.dep_) + \" \"\n",
    "    return res\n",
    "\n",
    "def dep_encoder_similarity(dep_1, dep_2):\n",
    "    res = 0\n",
    "    dep1 = dep_1\n",
    "    dep2 = dep_2\n",
    "    if len(dep1) > len(dep2):\n",
    "        dep1 = dep_2\n",
    "        dep2 = dep_1\n",
    "    for i, word in enumerate(dep1):\n",
    "        if word == dep2[i]:\n",
    "            res += 1\n",
    "    return res/(max(len(dep1), len(dep2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62acd119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def pos_extractor(pos):\n",
    "    if pos == \"NOUN\":\n",
    "        return \"n\"\n",
    "    if pos == \"VERB\":\n",
    "        return \"v\"\n",
    "    if pos == \"ADJ\":\n",
    "        return \"a\"\n",
    "    if pos == \"ADV\":\n",
    "        return \"r\"\n",
    "    else:\n",
    "        return \"o\"\n",
    "    \n",
    "    \n",
    "def get_index(sent, words):\n",
    "    ps = PorterStemmer()\n",
    "    words = [ps.stem(w) for w in words]\n",
    "    for i, word in enumerate(sent.split()):\n",
    "        if ps.stem(word) in words:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5bffc939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b c ', 0.04000000000000001), ('b d ', 0.04000000000000001)]\n"
     ]
    }
   ],
   "source": [
    "def all_subsets(lst1, lst2):\n",
    "    res = []\n",
    "    for item1 in lst1:\n",
    "        for item2 in lst2:\n",
    "            f = item1.copy()\n",
    "            f.append(item2)\n",
    "            res.append(f)\n",
    "    return res\n",
    "\n",
    "def paraphrase(to_be_replaced):\n",
    "    subsets = []\n",
    "    for i in range(len(to_be_replaced[0])):\n",
    "        subsets.append([i])\n",
    "    for i, replace in enumerate(to_be_replaced):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        a = [i for i in range(len(replace))]\n",
    "        subsets = all_subsets(subsets, a)\n",
    "    res = []\n",
    "    for subset in subsets:\n",
    "        text = \"\"\n",
    "        score = 1.0\n",
    "        for i, word_index in enumerate(subset):\n",
    "            text += to_be_replaced[i][word_index][0]\n",
    "            text += \" \"\n",
    "            score*=to_be_replaced[i][word_index][1]\n",
    "        sent = tuple((text, score))\n",
    "        res.append(sent)\n",
    "    sorted_list = sorted(res, key=lambda x: x[1], reverse = True) \n",
    "    return sorted_list\n",
    "        \n",
    "\n",
    "#print(all_subsets([[0, 1], [0, 2]], [0, 1, 2]))\n",
    "\n",
    "print(paraphrase([[tuple((\"b\", 0.2))], [tuple((\"c\", 0.2)), tuple((\"d\", 0.2))]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9b39c6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('he', 1.0)], [('can', 1.0)], [('achieve', 1.0), ('accomplish', 1.0), ('attain', 1.0), ('reach', 1.0)], [('his', 1.0)], [('goal', 1.0), ('finish', 1.0), ('destination', 1.0)]]\n",
      "[('he can achieve his goal ', 1.0), ('he can achieve his finish ', 1.0), ('he can achieve his destination ', 1.0), ('he can accomplish his goal ', 1.0), ('he can accomplish his finish ', 1.0), ('he can accomplish his destination ', 1.0), ('he can attain his goal ', 1.0), ('he can attain his finish ', 1.0), ('he can attain his destination ', 1.0), ('he can reach his goal ', 1.0), ('he can reach his finish ', 1.0), ('he can reach his destination ', 1.0)]\n"
     ]
    }
   ],
   "source": [
    "to_be_replaced = []\n",
    "doc = nlp(\"he can achieve his goal\")\n",
    "# doc = nlp(\"he goes to work every day\")\n",
    "# doc = nlp(\"he loves playing football\")\n",
    "# doc = nlp(\"this process includes many types of activities\")\n",
    "# doc = nlp(\"do you like to have a cup of tea\")\n",
    "# doc = nlp(\"hw loves his job\")\n",
    "\n",
    "#doc = nlp(\"I want to win this game\")\n",
    "doc = nlp(encoder.encode(doc))\n",
    "\n",
    "for i, token in enumerate(doc):\n",
    "    \n",
    "    if pos_extractor(token.pos_) == \"o\":\n",
    "        to_be_replaced.append([tuple((token.text, 1.0))])\n",
    "        continue\n",
    "    synsets = wn.synsets(token.text, pos = pos_extractor(token.pos_))\n",
    "    to_be_appended = [tuple((token.text, 1.0))]\n",
    "    new_words = set(token.text)\n",
    "    new_words.add(token.lemma_)\n",
    "    for synset in synsets:\n",
    "        names = [x.name() for x in synset.lemmas()]        \n",
    "        for example in synset.examples():\n",
    "            ind_head = get_index(encoder.encode(nlp(example)), names)\n",
    "            if ind_head is not None:\n",
    "                dep_example = dep_encoder(nlp(encoder.encode(nlp(example))), ind_head)\n",
    "                dep_original = dep_encoder(doc, i)\n",
    "                if len(dep_original) > 1:\n",
    "                    simi = dep_encoder_similarity(dep_example, dep_original)\n",
    "                    if simi > 0.5:\n",
    "                        for name in names:\n",
    "                            if name not in new_words:\n",
    "                                new_words.add(name)\n",
    "                                to_be_appended.append(tuple((name, simi)))\n",
    "    to_be_replaced.append(to_be_appended)\n",
    "print(to_be_replaced)\n",
    "print(paraphrase(to_be_replaced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5c436ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('he', 1.0)], [('loves', 1.0), ('enjoy', 0.43151522449495616), ('know', 0.39090325277610716), ('screw', 0.3693842734242532), ('fuck', 0.3747362351408903), ('jazz', 0.3108570027979174), ('bed', 0.3044436655213504), ('bang', 0.3327514086942118)], [('his', 1.0)], [('job', 1.0), ('occupation', 0.4365978158609427), ('business', 0.4491671778335864), ('line', 0.33719778558596203), ('task', 0.4419834812851823), ('problem', 0.40334215120539607)]]\n",
      "[('he loves his job ', 1.0), ('he loves his business ', 0.4491671778335864), ('he loves his task ', 0.4419834812851823), ('he loves his occupation ', 0.4365978158609427), ('he enjoy his job ', 0.43151522449495616), ('he loves his problem ', 0.40334215120539607), ('he know his job ', 0.39090325277610716), ('he fuck his job ', 0.3747362351408903), ('he screw his job ', 0.3693842734242532), ('he loves his line ', 0.33719778558596203), ('he bang his job ', 0.3327514086942118), ('he jazz his job ', 0.3108570027979174), ('he bed his job ', 0.3044436655213504), ('he enjoy his business ', 0.19382247557862592), ('he enjoy his task ', 0.1907226011498377), ('he enjoy his occupation ', 0.18839860452524224), ('he know his business ', 0.1755809108554131), ('he enjoy his problem ', 0.17404827892567504), ('he know his task ', 0.17277278050768544), ('he know his occupation ', 0.17066750637498637), ('he fuck his business ', 0.16831921717021692), ('he screw his business ', 0.16591529163008162), ('he fuck his task ', 0.16562722577127334), ('he fuck his occupation ', 0.16360902178646533), ('he screw his task ', 0.16326174710004906), ('he screw his occupation ', 0.1612723669904102), ('he know his problem ', 0.15766775888790177), ('he fuck his problem ', 0.15114691921633783), ('he bang his business ', 0.14946101116332944), ('he screw his problem ', 0.1489882474643805), ('he bang his task ', 0.1470706260172162), ('he enjoy his line ', 0.14550597814632849), ('he bang his occupation ', 0.1452785382605448), ('he jazz his business ', 0.13962676265654783), ('he jazz his task ', 0.13739366027850117), ('he bed his business ', 0.13674610205153728), ('he jazz his occupation ', 0.1357194884666497), ('he bed his task ', 0.13455907114234805), ('he bang his problem ', 0.13421266899934933), ('he bed his occupation ', 0.13291943941932097), ('he know his line ', 0.1318117112144529), ('he fuck his line ', 0.12636022866832858), ('he jazz his problem ', 0.12538173222577384), ('he screw his line ', 0.1245555590289377), ('he bed his problem ', 0.12279496297223753), ('he bang his line ', 0.11220303816229767), ('he jazz his line ', 0.10482029297734695), ('he bed his line ', 0.10265772984947265)]\n"
     ]
    }
   ],
   "source": [
    "to_be_replaced = []\n",
    "doc = nlp(\"he insisted to achieve his goal\")\n",
    "doc = nlp(\"he loves his job\")\n",
    "\n",
    "doc = nlp(encoder.encode(doc))\n",
    "tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(doc.text, tokenizer)\n",
    "list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
    "for i, token in enumerate(doc):\n",
    "    if pos_extractor(token.pos_) == \"o\":\n",
    "        to_be_replaced.append([tuple((token.text, 1.0))])\n",
    "        continue\n",
    "    if tokenized_text.count(token.text) > 0:\n",
    "        word_index = tokenized_text.index(token.text)\n",
    "        word_embedding = list_token_embeddings[word_index]\n",
    "    else:\n",
    "        to_be_replaced.append([tuple((token.text, 1.0))])\n",
    "        continue\n",
    "    synsets = wn.synsets(token.text, pos = pos_extractor(token.pos_))\n",
    "    to_be_appended = [tuple((token.text, 1.0))]\n",
    "    new_words = set(token.text)\n",
    "    new_words.add(token.lemma_)\n",
    "    for synset in synsets:\n",
    "        names = [x.name() for x in synset.lemmas()]\n",
    "        for name in names:\n",
    "            context = doc.text.split(' ')\n",
    "            context[i] = name\n",
    "            tokenized_text_example, tokens_tensor_example, segments_tensors_example = bert_text_preparation(str(context), tokenizer)\n",
    "            list_token_embeddings_example = get_bert_embeddings(tokens_tensor_example, segments_tensors_example, model)\n",
    "            if tokenized_text_example.count(name) > 0:\n",
    "                word_index_example = tokenized_text_example.index(name)\n",
    "                word_embedding_example = list_token_embeddings_example[word_index_example]\n",
    "                if name not in new_words:\n",
    "                    to_be_appended.append(tuple((name, 1 - cosine(word_embedding, word_embedding_example))))\n",
    "\n",
    "    to_be_replaced.append(to_be_appended)\n",
    "print(to_be_replaced)\n",
    "print(paraphrase(to_be_replaced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6294e457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_embeddings(embedding):\n",
    "    words = sorted(embeddings_dict.keys(), key=lambda word: spatial.distance.cosine(embeddings_dict[word], embedding))\n",
    "    dists = [1 - spatial.distance.cosine(embeddings_dict[word], embedding) for word in words]\n",
    "    return words[:5], dists[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35647c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from scipy import spatial\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4253ae1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"he insisted to achieve his goal\")\n",
    "doc = nlp(\"the king loves his job\")\n",
    "\n",
    "doc = nlp(encoder.encode(doc))\n",
    "embeddings_dict = {}\n",
    "line = doc.text\n",
    "with open(\"glove.6B.300d.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3a28cb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he loves his job \n"
     ]
    }
   ],
   "source": [
    "to_be_replaced = []\n",
    "print(doc)\n",
    "for word in doc.text.split(' '):\n",
    "    if word not in stop_words:\n",
    "        if word in embeddings_dict:\n",
    "            to_be_appended = find_closest_embeddings(embeddings_dict[word])\n",
    "            lst = []\n",
    "            for i in range(len(to_be_appended[0])):\n",
    "                lst.append(tuple((to_be_appended[0][i], to_be_appended[1][i])))\n",
    "        else:\n",
    "            lst = [tuple((word, 1.0))]\n",
    "    else:\n",
    "        lst = [tuple((word, 1.0))]\n",
    "\n",
    "    \n",
    "    to_be_replaced.append(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5b753e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('he', 1.0)], [('loves', 1.0), ('hates', 0.7345002889633179), ('likes', 0.6868317127227783), ('everybody', 0.6422836184501648), ('love', 0.6420262455940247)], [('his', 1.0)], [('job', 1.0), ('jobs', 0.757643461227417), ('doing', 0.6103168725967407), ('work', 0.60713791847229), ('working', 0.6018442511558533)], [('', 1.0)]]\n"
     ]
    }
   ],
   "source": [
    "print(to_be_replaced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ed776509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('he loves his job  ', 1.0), ('he loves his jobs  ', 0.757643461227417), ('he hates his job  ', 0.7345002889633179), ('he likes his job  ', 0.6868317127227783), ('he everybody his job  ', 0.6422836184501648), ('he love his job  ', 0.6420262455940247), ('he loves his doing  ', 0.6103168725967407), ('he loves his work  ', 0.60713791847229), ('he loves his working  ', 0.6018442511558533), ('he hates his jobs  ', 0.5564893412027061), ('he likes his jobs  ', 0.5203735561080407), ('he everybody his jobs  ', 0.4866219837722525), ('he love his jobs  ', 0.4864269869107005), ('he hates his doing  ', 0.4482779192814945), ('he hates his work  ', 0.44594297655848436), ('he hates his working  ', 0.4420547763848859), ('he likes his doing  ', 0.4191849829092291), ('he likes his work  ', 0.4170015764032655), ('he likes his working  ', 0.41336571781373266), ('he everybody his doing  ', 0.39199652933262286), ('he love his doing  ', 0.3918394503359721), ('he everybody his work  ', 0.3899547391746836), ('he love his work  ', 0.3897984783545354), ('he everybody his working  ', 0.3865547033758112), ('he love his working  ', 0.3863998050019397)]\n"
     ]
    }
   ],
   "source": [
    "print(paraphrase(to_be_replaced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d5b675",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
